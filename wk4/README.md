## Intro

This folder has the assignments for week 4. Week 4 talks about neural networks and how perceptron's take in weights as input and can predict the probability of some supervised problem using the sigmoid function. We can use it to make AND, OR, gates using 2 layer neural networks; however, a more complex network (3 layers) is required for an XOR or XNOR gate.

I learnt a lot from this week - it is understandably just supervised learning but predicting text using given datasets is pretty cool in-and-of-itself! 

## Notes

 - Understanding week 4 was a little tough in the beginning; however, after going through the lectures and some additional googling definitely help me out with the concepts. 
 - For Neural Networks, the parameters (weights) are already provided so that made it easy to compute the hypothesis. 
 - OneVsAll classification is really interesting! Taking the example of handwriting recognition itself, after training your data sets, given new data, you can classify it as anything from 0-9; Neural Networks make the computation much easier.
 - The Neural Newtorks method produced greater accuracy - but that's probably just how the data sets were trained (i.e. the parameters were weighted) maybe by some method other than gradient descent. 

## Topics you'll learn
 - Neural Networks Architecture
 - OneVsAll classification
 - Regularization on multiclass logistic regression
