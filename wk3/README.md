## Intro

This folder has the assignments for week 3. It discusses logistic regression (which is, essentially, just classification) and it also covered some concepts of regularization. Regularization is pretty good. First, you add more features by using the same data points you have and just making your decision boundary more expressive so it can adjust to varying data sets. 

Some cases where it's extensively used: classifying emails as spam or not. Classifying some groups of students and basically grouping into binary groups. 

## Topics you'll learn
  - Logistic regression
  - Classification
  - Regularization (using lambda to minimize feature values)
  - Multiclass classification: One-vs-all or One-vs-rest
  - Gradient Descent 
  - Preventing overfitting by increasing number of features
    - This allows the decision boundary to be more expressive. 
    - which allows it to adjust to any data-set by adjusting the theta values (that's pretty cool!)
